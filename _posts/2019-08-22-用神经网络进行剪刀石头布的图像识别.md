---
typora-root-url: ..
---

# 用神经网络进行剪刀石头布的图像识别

# (rock_paper_scissors)

## 1、下载数据集
```python
!wget --no-check-certificate \
    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip \
    -O /tmp/rps.zip
  
!wget --no-check-certificate \
    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip \
    -O /tmp/rps-test-set.zip
```
### 2、解压数据集
```python
import os
import zipfile

local_zip = '/tmp/rps.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp/')
zip_ref.close()

local_zip = '/tmp/rps-test-set.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp/')
zip_ref.close()
````

```python
rock_dir = os.path.join('/tmp/rps/rock')
paper_dir = os.path.join('/tmp/rps/paper')
scissors_dir = os.path.join('/tmp/rps/scissors')

# 输出数据集图片总数
print('total training rock images:', len(os.listdir(rock_dir)))
print('total training paper images:', len(os.listdir(paper_dir)))
print('total training scissors images:', len(os.listdir(scissors_dir)))

# 输出每个类别的前十张图像文件名
rock_files = os.listdir(rock_dir)
print(rock_files[:10])

paper_files = os.listdir(paper_dir)
print(paper_files[:10])

scissors_files = os.listdir(scissors_dir)
print(scissors_files[:10])
```
### 3、使用`matpotlib.pyplot`显示输出每个类别的前两张图像
```python
%matplotlib inline

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

pic_index = 2

next_rock = [os.path.join(rock_dir, fname) 
                for fname in rock_files[pic_index-2:pic_index]]
next_paper = [os.path.join(paper_dir, fname) 
                for fname in paper_files[pic_index-2:pic_index]]
next_scissors = [os.path.join(scissors_dir, fname) 
                for fname in scissors_files[pic_index-2:pic_index]]

for i, img_path in enumerate(next_rock+next_paper+next_scissors):
  #print(img_path)
  img = mpimg.imread(img_path)
  plt.imshow(img)
  plt.axis('Off')
  plt.show()
```

![image-20200227173544776](./styles/images/typora-images/image-20200227173544776.png)

![image-20200227173558065](./styles/images/typora-images/image-20200227173558065.png)

![image-20200227173616508](./styles/images/typora-images/image-20200227173616508.png)

![image-20200227173632375](./styles/images/typora-images/image-20200227173632375.png)

![image-20200227173647776](./styles/images/typora-images/image-20200227173647776.png)

![image-20200227173700576](./styles/images/typora-images/image-20200227173700576.png)

### 4、训练代码
```python
import tensorflow as tf
# 导入图像预处理
import keras_preprocessing
from keras_preprocessing import image
# 导入图像预处理、图像生成器
from keras_preprocessing.image import ImageDataGenerator
# 训练集数据增强
TRAINING_DIR = "/tmp/rps/"
training_datagen = ImageDataGenerator(
    # 重放缩因子,默认为None. 如果为None或0则不进行放缩,否则会将该数值乘到数据上(在应用其他变换之前)
      rescale = 1./255,
    # 整数，数据增强时图片随机转动的角度范围
	  rotation_range=40,
    # 浮点数，图片宽度的某个比例，数据增强时图片水平偏移的幅度
      width_shift_range=0.2,
    # 浮点数，图片高度的某个比例，数据增强时图片竖直偏移的幅度
      height_shift_range=0.2,
    # 浮点数，剪切强度（逆时针方向的剪切变换角度）
      shear_range=0.2,
    
      zoom_range=0.2,
      horizontal_flip=True,
      fill_mode='nearest')

'''
参数：
featurewise_center：布尔值，使输入数据集去中心化（均值为0）, 按feature执行
samplewise_center：布尔值，使输入数据的每个样本均值为0
featurewise_std_normalization：布尔值，将输入除以数据集的标准差以完成标准化, 按feature执行
samplewise_std_normalization：布尔值，将输入的每个样本除以其自身的标准差
zca_whitening：布尔值，对输入数据施加ZCA白化
zca_epsilon: ZCA使用的eposilon，默认1e-6
rotation_range：整数，数据提升时图片随机转动的角度范围
width_shift_range：浮点数，图片宽度的某个比例，数据提升时图片水平偏移的幅度
height_shift_range：浮点数，图片高度的某个比例，数据提升时图片竖直偏移的幅度
shear_range：浮点数，剪切强度（逆时针方向的剪切变换角度）
zoom_range：浮点数或形如[lower,upper]的列表，随机缩放的幅度，若为浮点数，则相当于[lower,upper] = [1 - zoom_range, 1+zoom_range]
channel_shift_range：浮点数，随机通道偏移的幅度
fill_mode：；‘constant’，‘nearest’，‘reflect’或‘wrap’之一，当进行变换时超出边界的点将根据本参数给定的方法进行处理
cval：浮点数或整数，当fill_mode=constant时，指定要向超出边界的点填充的值
horizontal_flip：布尔值，进行随机水平翻转
vertical_flip：布尔值，进行随机竖直翻转
rescale: 重放缩因子,默认为None. 如果为None或0则不进行放缩,否则会将该数值乘到数据上(在应用其他变换之前)
preprocessing_function: 将被应用于每个输入的函数。该函数将在图片缩放和数据提升之后运行。该函数接受一个参数，为一张图片（秩为3的numpy array），并且输出一个具有相同shape的numpy array
data_format：字符串，“channel_first”或“channel_last”之一，代表图像的通道维的位置。该参数是Keras 1.x中的image_dim_ordering，“channel_last”对应原本的“tf”，“channel_first”对应原本的“th”。以128x128的RGB图像为例，“channel_first”应将数据组织为（3,128,128），而“channel_last”应将数据组织为（128,128,3）。该参数的默认值是~/.keras/keras.json中设置的值，若从未设置过，则为“channel_last”
'''
# 验证集数据增强
VALIDATION_DIR = "/tmp/rps-test-set/"
validation_datagen = ImageDataGenerator(rescale = 1./255)

train_generator = training_datagen.flow_from_directory(
	TRAINING_DIR,
	target_size=(150,150),
	class_mode='categorical'
)


validation_generator = validation_datagen.flow_from_directory(
	VALIDATION_DIR,
	target_size=(150,150),
	class_mode='categorical'
)

model = tf.keras.models.Sequential([
    # Note the input shape is the desired size of the image 150x150 with 3 bytes color
    # This is the first convolution
    # 第一层卷积、池化层（注意输入input_shape的大小
    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    # The second convolution
    # 第二层卷积、池化层
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The third convolution
    # 第三层卷积、池化层
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The fourth convolution
    # 第四层卷积、池化层
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # Flatten the results to feed into a DNN
    # 将多层卷积结果展平后传入到神经网络（随机dropout设置为0.5）
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.5),
    # 512 neuron hidden layer
    # 隐藏层由512个人工神经元构成
    tf.keras.layers.Dense(512, activation='relu'),
    # 最后进行激活函数为softmax的3分类
    tf.keras.layers.Dense(3, activation='softmax')
])

# 列出模型的层结构
model.summary()
# 编译模型
model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
# 训练模型
history = model.fit_generator(train_generator, epochs=25, validation_data = validation_generator, verbose = 1)
# 保存模型
model.save("rps.h5")
```

输出：

```python
Found 2520 images belonging to 3 classes.
Found 372 images belonging to 3 classes.
Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_20 (Conv2D)           (None, 148, 148, 64)      1792      
_________________________________________________________________
max_pooling2d_20 (MaxPooling (None, 74, 74, 64)        0         
_________________________________________________________________
conv2d_21 (Conv2D)           (None, 72, 72, 64)        36928     
_________________________________________________________________
max_pooling2d_21 (MaxPooling (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_22 (Conv2D)           (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_22 (MaxPooling (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_23 (Conv2D)           (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_23 (MaxPooling (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_5 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dropout_5 (Dropout)          (None, 6272)              0         
_________________________________________________________________
dense_10 (Dense)             (None, 512)               3211776   
_________________________________________________________________
dense_11 (Dense)             (None, 3)                 1539      
=================================================================
Total params: 3,473,475
Trainable params: 3,473,475
Non-trainable params: 0
_________________________________________________________________
Epoch 1/25
79/79==============================] - 20s 257ms/step - loss: 1.2037 - acc: 0.3786 - val_loss: 1.0084 - val_acc: 0.6344
Epoch 2/25
79/79==============================] - 19s 242ms/step - loss: 0.8781 - acc: 0.6000 - val_loss: 0.3174 - val_acc: 0.9946
Epoch 3/25
79/79==============================] - 20s 250ms/step - loss: 0.5636 - acc: 0.7595 - val_loss: 0.1306 - val_acc: 1.0000
Epoch 4/25
79/79==============================] - 19s 243ms/step - loss: 0.4033 - acc: 0.8397 - val_loss: 0.2548 - val_acc: 0.8414
Epoch 5/25
79/79==============================] - 19s 240ms/step - loss: 0.3114 - acc: 0.8897 - val_loss: 0.0518 - val_acc: 0.9866
Epoch 6/25
79/79==============================] - 20s 251ms/step - loss: 0.2161 - acc: 0.9278 - val_loss: 0.1461 - val_acc: 0.9409
Epoch 7/25
79/79==============================] - 19s 246ms/step - loss: 0.2107 - acc: 0.9282 - val_loss: 0.0579 - val_acc: 0.9892
Epoch 8/25
79/79==============================] - 19s 243ms/step - loss: 0.1796 - acc: 0.9345 - val_loss: 0.0487 - val_acc: 0.9812
Epoch 9/25
79/79==============================] - 20s 252ms/step - loss: 0.1681 - acc: 0.9468 - val_loss: 0.0209 - val_acc: 0.9946
Epoch 10/25
79/79==============================] - 20s 257ms/step - loss: 0.1286 - acc: 0.9532 - val_loss: 0.1703 - val_acc: 0.9328
Epoch 11/25
79/79==============================] - 19s 241ms/step - loss: 0.1457 - acc: 0.9532 - val_loss: 0.0287 - val_acc: 0.9919
Epoch 12/25
79/79==============================] - 19s 241ms/step - loss: 0.1053 - acc: 0.9687 - val_loss: 0.0737 - val_acc: 0.9704
Epoch 13/25
79/79==============================] - 20s 249ms/step - loss: 0.1216 - acc: 0.9603 - val_loss: 0.0290 - val_acc: 0.9866
Epoch 14/25
79/79==============================] - 19s 236ms/step - loss: 0.1651 - acc: 0.9532 - val_loss: 0.0918 - val_acc: 0.9758
Epoch 15/25
79/79==============================] - 19s 236ms/step - loss: 0.0996 - acc: 0.9659 - val_loss: 0.1870 - val_acc: 0.8978
Epoch 16/25
79/79==============================] - 20s 248ms/step - loss: 0.0804 - acc: 0.9702 - val_loss: 0.0228 - val_acc: 0.9839
Epoch 17/25
79/79==============================] - 19s 241ms/step - loss: 0.1079 - acc: 0.9659 - val_loss: 0.0448 - val_acc: 0.9785
Epoch 18/25
79/79==============================] - 19s 240ms/step - loss: 0.1005 - acc: 0.9667 - val_loss: 0.0431 - val_acc: 0.9812
Epoch 19/25
79/79==============================] - 19s 244ms/step - loss: 0.0983 - acc: 0.9698 - val_loss: 0.0627 - val_acc: 0.9785
Epoch 20/25
79/79==============================] - 20s 249ms/step - loss: 0.0830 - acc: 0.9738 - val_loss: 0.3355 - val_acc: 0.8575
Epoch 21/25
79/79==============================] - 19s 245ms/step - loss: 0.1141 - acc: 0.9647 - val_loss: 0.0584 - val_acc: 0.9731
Epoch 22/25
79/79==============================] - 19s 243ms/step - loss: 0.0803 - acc: 0.9750 - val_loss: 0.0380 - val_acc: 0.9812
Epoch 23/25
79/79==============================] - 20s 253ms/step - loss: 0.0762 - acc: 0.9754 - val_loss: 0.3842 - val_acc: 0.9167
Epoch 24/25
79/79==============================] - 19s 246ms/step - loss: 0.0781 - acc: 0.9758 - val_loss: 0.0176 - val_acc: 0.9892
Epoch 25/25
79/79==============================] - 19s 237ms/step - loss: 0.0708 - acc: 0.9810 - val_loss: 0.1145 - val_acc: 0.9543
```

绘制训练过程：

```python
import matplotlib.pyplot as plt
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend(loc=0)
plt.figure()


plt.show()
```

![image-20200302095752972](./styles/images/typora-images/image-20200302095752972.png)



