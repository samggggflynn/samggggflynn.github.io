---
layout: post
#标题配置
title: 数据挖掘的9大成熟技术和商业应用
#时间配
date:   2019-05-11 21:17:00 +0800
#大类配置
categories: 机器学习
#小类配置
tag: 数据挖掘
---

* content
{:toc}

---
<iframe src="https://open.spotify.com/embed/track/2Fxmhks0bxGSBdJ92vM42m" width="300" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

<iframe width="960" height="540" src="https://www.youtube.com/watch?v=yd_OSTzK0d0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

数据挖掘的9大成熟技术和商业应用
================

* * *

基于数据挖掘的9大主要成熟技术以及在数据化运营中的主要应用：  
1、决策树  
2、神经网络  
3、回归  
4、关联规则  
5、聚类  
6、贝叶斯分类  
7、支持向量机  
8、主成分分析  
9、假设检验

### [](#1-决策树 "1　决策树")1　决策树

决策树（Decision Tree）是一种非常成熟的、普遍采用的数据挖掘技术。之所以称为树，是因为其建模过程类似一棵树的成长过程，即从根部开始，到树干，到分枝，再到细枝末节的分叉，最终生长出一片片的树叶。在决策树里，所分析的数据样本先是集成为一个树根，然后经过层层分枝，最终形成若干个结点，每个结点代表一个结论。

决策树算法之所以在数据分析挖掘应用中如此流行，主要原因在于决策树的构造不需要任何领域的知识，很适合探索式的知识发掘，并且可以处理高维度的数据。在众多的数据挖掘、统计分析算法中，决策树最大的优点在于它所产生的一系列从树根到树枝（或树叶）的规则，可以很容易地被分析师和业务人员理解，而且这些典型的规则甚至不用整理（或稍加整理），就是现成的可以应用的业务优化策略和业务优化路径。另外，决策树技术对数据的分布甚至缺失非常宽容，不容易受到极值的影响。

目前，最常用的3种决策树算法分别是`CHAID、CART`和`ID3（包括后来的C4.5，乃至C5.0）`。

CHAID(Chi-square Automatic Interaction Detector)算法的历史较长，中文简称为卡方自动相互关系检测。CHAID依据局部最优原则，利用卡方检验来选择对因变量最有影响的自变量，CHAID应用的前提是因变量为类别型变量（Category）。

CART(Classification and Regression Tree)算法产生于20世纪80年代中期，中文简称为分类与回归树，CART的分割逻辑与CHAID相同，每一层的划分都是基于对所有自变量的检验和选择上的。但是，CART采用的检验标准不是卡方检验，而是基尼系数（Gini）等不纯度的指标。两者最大的区别在于CHAID采用的是局部最优原则，即结点之间互不相干，一个结点确定了之后，下面的生长过程完全在结点内进行。而CART则着眼于总体优化，即先让树尽可能地生长，然后再回过头来对树进行修剪（Prune），这一点非常类似统计分析中回归算法里的反向选择（Backward Selection）。CART所生产的决策树是二分的，每个结点只能分出两枝，并且在树的生长过程中，同一个自变量可以反复使用多次（分割），这些都是不同于CHAID的特点。另外，如果是自变量存在数据缺失（Missing）的情况，CART的处理方式将会是寻找一个替代数据来代替（填充）缺失值，而CHAID则是把缺失数值作为单独的一类数值。

`ID3`（Iterative Dichotomiser）算法与CART是同一时期产生的，中文简称为迭代的二分器，其最大的特点在于自变量的挑选标准是：基于信息增益的度量选择具有最高信息增益的属性作为结点的分裂（分割）属性，其结果就是对分割后的结点进行分类所需的信息量最小，这也是一种划分纯度的思想。至于之后发展起来的`C4.5`可以理解为ID3的发展版（后继版），两者的主要区别在于C4.5采用信息`增益率`（Gain Ratio）代替了ID3中的信息增益度量，如此替换的主要原因是信息增益度量有个缺点，就是倾向于选择具有大量值的属性。这里给个极端的例子，对于Member\_Id的划分，每个Id都是一个最纯的组，但是这样的划分没有任何实际意义。而C4.5所采用的信息增益率就可以较好地克服这个缺点，它在信息增益的基础上，增加了一个分裂信息（SplitInformation）对其进行规范化约束。

决策树技术在数据化运营中的主要用途体现在：作为分类、预测问题的典型支持技术，它在用户划分、行为预测、规则梳理等方面具有广泛的应用前景，决策树甚至可以作为其他建模技术前期进行变量筛选的一种方法，即通过决策树的分割来筛选有效地输入自变量。

### [](#2-神经网络 "2　神经网络")2　神经网络

神经网络（Neural Network）是通过数学算法来模仿人脑思维的，它是数据挖掘中机器学习的典型代表。神经网络是人脑的抽象计算模型，我们知道人脑中有数以百亿个神经元（人脑处理信息的微单元），这些神经元之间相互连接，使得人的大脑产生精密的逻辑思维。而数据挖掘中的“神经网络”也是由大量并行分布的人工神经元（微处理单元）组成的，它有通过调整连接强度从经验知识中进行学习的能力，并可以将这些知识进行应用。

简单来讲，“神经网络”就是通过输入多个非线性模型以及不同模型之间的加权互联（加权的过程在隐蔽层完成），最终得到一个输出模型。其中，隐蔽层所包含的就是非线性函数。

目前最主流的“神经网络”算法是`反馈传播（Backpropagation）`，该算法在多层前向型（Multilayer Feed-Forward）神经网络上进行学习，而多层前向型神经网络又是由一个输入层、一个或多个隐蔽层以及一个输出层组成的，“神经网络”的典型结构如图所示。  
[![](https://ws2.sinaimg.cn/large/006tKfTcly1fq81j3ffllj30s20dawfw.jpg)](https://ws2.sinaimg.cn/large/006tKfTcly1fq81j3ffllj30s20dawfw.jpg)

由于“神经网络”拥有特有的大规模并行结构和信息的并行处理等特点，因此它具有良好的`自适应性`、`自组织性`和`高容错性`，并且具有较强的学习、记忆和识别功能。目前神经网络已经在信号处理、模式识别、专家系统、预测系统等众多领域中得到广泛的应用。

“神经网络”的主要缺点就是其知识和`结果的不可解释性`，没有人知道隐蔽层里的非线性函数到底是如何处理自变量的，“神经网络”应用中的产出物在很多时候让人看不清其中的逻辑关系。但是，它的这个缺点并没有影响该技术在数据化运营中的广泛应用，甚至可以这样认为，正是因为其结果具有不可解释性，反而更有可能促使我们发现新的没有认识到的规律和关系。

在利用“神经网络”技术建模的过程中，有以下5个因素对模型结果有着重大影响：

❑层数。

❑每层中输入变量的数量。

❑联系的种类。

❑联系的程度。

❑转换函数，又称激活函数或挤压函数。

“神经网络”技术在数据化运营中的主要用途体现在：作为`分类`、`预测`问题的重要技术支持，在用户划分、行为预测、营销响应等诸多方面具有广泛的应用前景。

### [](#3-回归 "3　回归")3　回归

回归（Regression）分析包括线性回归（Linear Regression），这里主要是指多元线性回归和逻辑斯蒂回归（Logistic Regression）。其中，**在数据化运营中更多使用的是逻辑斯蒂回归**，它又包括响应预测、分类划分等内容。

多元线性回归主要描述一个因变量如何随着一批自变量的变化而变化，其回归公式（回归方程）就是因变量与自变量关系的数据反映。因变量的变化包括两部分：系统性变化与随机变化，其中，系统性变化是由自变量引起的（自变量可以解释的），随机变化是不能由自变量解释的，通常也称作残值。

在用来估算多元线性回归方程中自变量系数的方法中，最常用的是最小二乘法，即找出一组对应自变量的相应参数，以使因变量的实际观测值与回归方程的预测值之间的总方差减到最小。

对多元线性回归方程的参数估计，是基于下列假设的：

❑输入变量是确定的变量，不是随机变量，而且输入的变量间无线性相关，即无共线性。

❑随机误差的期望值总和为零，即随机误差与自变量不相关。

❑随机误差呈现正态分布 \[1\]。

如果不满足上述假设，就不能用最小二乘法进行回归系数的估算了。

**逻辑斯蒂回归（Logistic Regression）相比于线性回归来说，在数据化运营中有更主流更频繁的应用**，主要是因为该分析技术可以很好地回答诸如预测、分类等数据化运营常见的分析项目主题。简单来讲，凡是预测“两选一”事件的可能性（比如，“响应”还是“不响应”；“买”还是“不买”；“流失”还是“不流失”），都可以采用逻辑斯蒂回归方程。

逻辑斯蒂回归预测的因变量是介于0和1之间的概率，如果对这个概率进行换算，就可以用线性公式描述因变量与自变量的关系了，具体公式如下：  
[![](https://ws4.sinaimg.cn/large/006tKfTcly1fq81p6q5gnj30al021t8k.jpg)](https://ws4.sinaimg.cn/large/006tKfTcly1fq81p6q5gnj30al021t8k.jpg)

与多元线性回归所采用的最小二乘法的参数估计方法相对应，最大似然法是逻辑斯蒂回归所采用的参数估计方法，其原理是找到这样一个参数，可以让样本数据所包含的观察值被观察到的可能性最大。这种寻找最大可能性的方法需要反复计算，对计算能力有很高的要求。最大似然法的优点是在大样本数据中参数的估值稳定、偏差小，估值方差小。

### [](#4-关联规则 "4　关联规则")4　关联规则

关联规则（Association Rule）是在数据库和数据挖掘领域中被发明并被广泛研究的一种重要模型，关联规则数据挖掘的主要目的是`找出数据集中的频繁模式（Frequent Pattern）`，即多次重复出现的模式和并发关系（Cooccurrence Relationships），`即同时出现的关系，频繁和并发关系也称作关联（Association）`。

应用关联规则最经典的案例就是购物篮分析（Basket Analysis），通过分析顾客购物篮中商品之间的关联，可以挖掘顾客的购物习惯，从而帮助零售商更好地制定有针对性的营销策略。

以下列举一个简单的关联规则的例子：

婴儿尿不湿→啤酒\[支持度=10%，置信度=70%\]

这个规则表明，在所有顾客中，有10%的顾客同时购买了婴儿尿不湿和啤酒，而在所有购买了婴儿尿不湿的顾客中，占70%的人同时还购买了啤酒。发现这个关联规则后，超市零售商决定把婴儿尿不湿和啤酒摆放在一起进行促销，结果明显提升了销售额，这就是发生在沃尔玛超市中“啤酒和尿不湿”的经典营销案例。

上面的案例是否让你对支持度和置信度有了一定的了解？事实上，支持度（Support）和置信度（Confidence）是衡量关联规则强度的两个重要指标，它们分别反映着所发现规则的有用性和确定性。其中`支持度：规则X→Y的支持度是指事物全集中包含X∪Y的事物百分比`。支持度主要衡量规则的有用性，如果支持度太小，则说明相应规则只是偶发事件。在商业实战中，偶发事件很可能没有商业价值；`置信度：规则X→Y的置信度是指既包含了X又包含了Y的事物数量占所有包含了X的事物数量的百分比`。置信度主要衡量规则的确定性（可预测性），如果置信度太低，那么从X就很难可靠地推断出Y来，置信度太低的规则在实践应用中也没有太大用处。

在众多的关联规则数据挖掘算法中，最著名的就是`Apriori`算法，该算法具体分为以下两步进行：

（1）生成所有的频繁项目集。一个频繁项目集（Frequent Itemset）是一个支持度高于最小支持度阀值（min-sup）的项目集。

（2）从频繁项目集中生成所有的可信关联规则。这里可信关联规则是指置信度大于最小置信度阀值（min-conf）的规则。

关联规则算法不但在数值型数据集的分析中有很大用途，而且在纯文本文档和网页文件中，也有着重要用途。比如发现单词间的并发关系以及Web的使用模式等，这些都是Web数据挖掘、搜索及推荐的基础。

### [](#5-聚类 "5　聚类")5　聚类

聚类（Clustering）分析有一个通俗的解释和比喻，那就是“物以类聚，人以群分”。针对几个特定的业务指标，可以将观察对象的群体按照相似性和相异性进行不同群组的划分。经过划分后，每个群组内部各对象间的相似度会很高，而在不同群组之间的对象彼此间将具有很高的相异度。

聚类分析的算法可以分为`划分的方法`（Partitioning Method）、`层次的方法`（Hierarchical Method）、基于密度的方法（Density-based Method）、基于网格的方法（Grid-based Method）、基于模型的方法（Model-based Method）等，其中，前面两种方法最为常用。

对于划分的方法（Partitioning Method），`当给定m个对象的数据集，以及希望生成的细分群体数量K后，即可采用这种方法将这些对象分成K组`（K≤m），使得每个组内对象是相似的，而组间的对象是相异的。最常用的划分方法是`K-Means`方法，其具体原理是：首先，随机选择K个对象，并且所选择的每个对象都代表一个组的初始均值或初始的组中心值；对剩余的每个对象，根据其与各个组初始均值的距离，将它们分配给最近的（最相似）小组；然后，重新计算每个小组新的均值；这个过程不断重复，直到所有的对象在K组分布中都找到离自己最近的组。

`层次的方法（Hierarchical Method）则是指依次让最相似的数据对象两两合并，这样不断地合并，最后就形成了一棵聚类树`。

聚类技术在数据分析和数据化运营中的主要用途表现在：既可以直接作为模型对观察对象进行群体划分，为业务方的精细化运营提供具体的细分依据和相应的运营方案建议，又可在数据处理阶段用作数据探索的工具，包括发现离群点、孤立点，数据降维的手段和方法，通过聚类发现数据间的深层次的关系等。

### [](#6-贝叶斯分类方法 "6　贝叶斯分类方法")6　贝叶斯分类方法

贝叶斯分类方法（Bayesian Classifier）是非常成熟的统计学分类方法，它主要用来预测类成员间关系的可能性。比如通过一个给定观察值的相关属性来判断其属于一个特定类别的概率。贝叶斯分类方法是基于贝叶斯定理的，已经有研究表明，朴素贝叶斯分类方法作为一种简单贝叶斯分类算法甚至可以跟决策树和神经网络算法相媲美。

贝叶斯定理的公式如下：  
[![](https://ws4.sinaimg.cn/large/006tKfTcly1fq820l60g6j307201smx0.jpg)](https://ws4.sinaimg.cn/large/006tKfTcly1fq820l60g6j307201smx0.jpg)

其中，X表示n个属性的测量描述；H为某种假设，比如假设某观察值X属于某个特定的类别C；对于分类问题，希望确定P(H|X)，即能通过给定的X的测量描述，来得到H成立的概率，也就是给出X的属性值，计算出该观察值属于类别C的概率。因为P(H|X)是后验概率（Posterior Probability），所以又称其为在条件X下，H的后验概率。

举例来说，假设数据属性仅限于用教育背景和收入来描述顾客，而X是一位硕士学历，收入10万元的顾客。假定H表示假设我们的顾客将购买苹果手机，则P(H|X)表示当我们知道顾客的教育背景和收入情况后，该顾客将购买苹果手机的概率；相反，P(X|H)则表示如果已知顾客购买苹果手机，则该顾客是硕士学历并且收入10万元的概率；而P(X)则是X的先验概率，表示顾客中的某个人属于硕士学历且收入10万元的概率；P(H)也是先验概率，只不过是任意给定顾客将购买苹果手机的概率，而不会去管他们的教育背景和收入情况。

从上面的介绍可见，相比于先验概率P(H)，后验概率P(H|X)基于了更多的信息（比如顾客的信息属性），而P(H)是独立于X的。

贝叶斯定理是朴素贝叶斯分类法（Naive Bayesian Classifier）的基础，如果给定数据集里有M个分类类别，通过朴素贝叶斯分类法，可以预测给定观察值是否属于具有最高后验概率的特定类别，也就是说，朴素贝叶斯分类方法预测X属于类别CiCi时，表示当且仅当  

![](http://latex.codecogs.com/svg.latex?\ P(Ci|X)＞P(Cj|X)1≤j≤m，j≠iP(Ci|X)＞P(Cj|X)1≤j≤m，j≠i)

此时如果最大化![](http://latex.codecogs.com/svg.latex?\ P(Ci|X)P(Ci|X))，其![](http://latex.codecogs.com/svg.latex?\ P(Ci|X)P(Ci|X))最大的类CiCi被称为最大后验假设，根据贝叶斯定理可知，由于P(X)P(X)对于所有的类别是均等的，因此只需要![](http://latex.codecogs.com/svg.latex?\ P(X|Ci)P(Ci)P(X|Ci)P(Ci))取最大即可。

为了预测一个未知样本XX的类别，可对每个类别CiCi估算相应的![](http://latex.codecogs.com/svg.latex?\ P(X|Ci)P(Ci)P(X|Ci)P(Ci))。样本XX归属于类别CiCi，当且仅当  

![](http://latex.codecogs.com/svg.latex?\ P(Ci|X)＞P(Cj|X)1≤j≤m，j≠iP(Ci|X)＞P(Cj|X)1≤j≤m，j≠i)

贝叶斯分类方法在数据化运营实践中主要用于分类问题的归类等应用场景。

### [](#7-支持向量机 "7　支持向量机")7　支持向量机

支持向量机（Support Vector Machine）是Vapnik等人于1995年率先提出的，是近年来机器学习研究的一个重大成果。与传统的神经网络技术相比，支持向量机不仅结构简单，而且各项技术的性能也明显提升，因此它成为当今机器学习领域的热点之一。

作为一种新的分类方法，支持向量机以结构风险最小为原则。在线性的情况下，就在原空间寻找两类样本的最优分类超平面。在非线性的情况下，它`使用一种非线性的映射，将原训练集数据映射到较高的维上`。在新的维上，它搜索线性最佳分离超平面。使用一个适当的对足够高维的非线性映射，两类数据总可以被超平面分开。

支持向量机的基本概念如下：

设给定的训练样本集为(x1,y1),(x2,y2),…,(xn,yn)(x1,y1),(x2,y2),…,(xn,yn)，其中![](http://latex.codecogs.com/svg.latex?\ xi∈Rn,y∈\[−1,1\]xi∈Rn,y∈\[−1,1\])。

再假设该训练集可被一个超平面线性划分，设该超平面记为![](http://latex.codecogs.com/svg.latex?\ (w,x)+b\=0(w,x)+b\=0)。

支持向量机的基本思想可用下图的两维情况举例说明。（线性可分情况下的最优分类线）  
[![](https://ws3.sinaimg.cn/large/006tKfTcly1fq82c0jkiaj30m50g3dgg.jpg)](https://ws3.sinaimg.cn/large/006tKfTcly1fq82c0jkiaj30m50g3dgg.jpg)

图中圆形和方形代表两类样本，H为分类线，H1、H2，分别为过各类样本中离分类线最近的样本并且平行于分类线的直线，它们之间的距离叫做`分类间隔（Margin）`。所谓的最优分类线就是要求分类线`不但能将两类正确分开（训练错误为0），而且能使分类间隔最大`。推广到高维空间，最优分类线就成了最优分类面。

其中，距离超平面最近的一类向量被称为`支持向量（Support Vector）`，`一组支持向量可以唯一地确定一个超平面`。通过学习算法，SVM可以自动寻找出那些对分类有较好区分能力的支持向量，由此构造出的分类器则可以最大化类与类的间隔，因而有较好的适应能力和较高的分类准确率。

支持向量机的缺点是训练数据较大，但是，它的优点也是很明显的——对于复杂的非线性的决策边界的建模能力高度准确，并且也不太容易过拟合 。

支持向量机主要用在预测、分类这样的实际分析需求场景中。

### [](#8-主成分分析 "8　主成分分析")8　主成分分析

严格意义上讲，主成分分析（Principal Components Analysis）属于传统的统计分析技术范畴，但是正如本章前面所阐述的，统计分析与数据挖掘并没有严格的分割，因此在数据挖掘实战应用中也常常会用到这种方式，从这个角度讲，主成分分析也是数据挖掘商业实战中常用的一种分析技术和数据处理技术。

主成分分析会通过线性组合将多个原始变量合并成若干个主成分，这样`每个主成分都变成了原始变量的线性组合`。这种转变的目的，一方面是可以大幅降低原始数据的维度，同时也在此过程中发现原始数据属性之间的关系。

主成分分析的主要步骤如下：

1）通常要先进行各变量的标准化工作，标准化的目的是将数据按照比例进行缩放，使之落入一个小的区间范围之内，从而让不同的变量经过标准化处理后可以有平等的分析和比较基础。

2）选择`协方差阵`或者`相关阵计算特征根`及对应的`特征向量`。

3）`计算方差贡献率`，并根据方差贡献率的阀值选取合适的主成分个数。

4）根据主成分载荷的大小对选择的主成分进行命名。

5）根据主成分载荷计算各个主成分的得分。

将主成分进行推广和延伸即成为`因子分析（Factor Analysis）`，因子分析在`综合原始变量信息的基础上将会力图构筑若干个意义较为明确的公因子`；也就是说，采用少数几个因子描述多个指标之间的联系，将比较密切的变量归为同一类中，每类变量即是一个因子。之所以称其为因子，是因为它们实际上是`不可测量的，只能解释`。

`主成分分析是因子分析的一个特例`，两者的区别和联系主要表现在以下方面：

❑主成分分析会把主成分表示成各个原始变量的线性组合，而因子分析则把原始变量表示成各个因子的线性组合。这个区别最直观也最容易记住。

❑主成分分析的重点在于解释原始变量的总方差，而因子分析的重点在于解释原始变量的协方差。

❑在主成分分析中，有几个原始变量就有几个主成分，而在因子分析中，因子个数可以根据业务场景的需要人为指定，并且指定的因子数量不同，则分析结果也会有差异。

❑在主成分分析中，给定的协方差矩阵或者相关矩阵的特征值是唯一时，主成分也是唯一的，但是在因子分析中，因子不是唯一的，并且通过旋转可以得到不同的因子。

主成分分析和因子分析在数据化运营实践中主要用于数据处理、降维、变量间关系的探索等方面，同时作为统计学里的基本而重要的分析工具和分析方法，它们在一些专题分析中也有着广泛的应用。

### [](#9-假设检验 "9　假设检验")9　假设检验

假设检验（Hypothesis Test）是现代统计学的基础和核心之一，其主要研究在一定的条件下，总体是否具备某些特定特征。

假设检验的基本原理就是小概率事件原理，即观测小概率事件在假设成立的情况下是否发生。如果在一次试验中，小概率事件发生了，那么说明假设在一定的显著性水平下不可靠或者不成立；如果在一次试验中，小概率事件没有发生，那么也只能说明没有足够理由相信假设是错误的，但是也并不能说明假设是正确的，因为无法收集到所有的证据来证明假设是正确的。

假设检验的结论是在一定的显著性水平下得出的。因此，当采用此方法观测事件并下结论时，有可能会犯错，这些错误主要有两大类：

❑第Ⅰ类错误：当原假设为真时，却否定它而犯的错误，即拒绝正确假设的错误，也叫弃真错误。犯第Ⅰ类错误的概率记为α，通常也叫α错误，α=1-置信度。

❑第Ⅱ类错误：当原假设为假时，却肯定它而犯的错误，即接受错误假设的错误，也叫纳伪错误。犯第Ⅱ类错误的概率记为β，通常也叫β错误。

上述这两类错误在其他条件不变的情况下是相反的，即α增大时，β就减小；α减小时，β就增大。α错误容易受数据分析人员的控制，因此在假设检验中，通常会先控制第Ⅰ类错误发生的概率α，具体表现为：在做假设检验之前先指定一个α的具体数值，通常取0.05，也可以取0.1或0.001。

在数据化运营的商业实践中，假设检验最常用的场景就是用于“运营效果的评估”上。

[原文](http://ihoge.cn/2018/DataMining.html)